---
title: 机器学习机试
date: 2022-04-11 10:12:00
updated: 2022-02-27 10:45:00
tag:
- leetcode
- python
---



## 1. 机器学习基础

### 1.1 CNN

1. CNN的输出尺寸

    输出尺寸=(输入尺寸-filter尺寸+2*padding）/stride+1，不能整除的部分去底

### 1.2 机器学习模型

1. **DBN**：深度信念网络、生成式模型

2. **CRF**：CRF模型是条件随机场（Conditional Random Field）的简称，是一种无向图模型，用于标注或分析序列数据，如自然语言文字或是生物序列。CRF模型使用无向图表示变量间的依赖关系，每个边对应一个特征函数，根据整个观察序列和当前状态来预测下一个状态。

    - **判别式模型（因为有“条件”）**

    - CRF模型相比于HMM模型，有以下优点：

        - 可以利用更多的特征信息来建立条件概率分布，提高了模型的表达能力和泛化能力。

        - 可以避免标注偏置问题（label bias problem），即某些状态节点由于连接较少或者转移概率较高而被过分偏好，导致预测结果不准确。

        - 可以使用前向后向算法进行高效的推理和学习，避免了HMM中的维特比算法可能出现的错误传播问题。

    - CRF模型相比于MEMM模型，有以下优点：

        - 不需要假设输出变量之间具有马尔可夫性质，可以捕捉**更长程的依赖关系**。

        - 不需要定义大量的**约束条件**，并且约束条件与样本数目无关。

    - **CRF缺点：训练代价大、复杂度高**

    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230316100508442.png" alt="image-20230316100508442" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230316100539268.png" alt="image-20230316100539268" style="zoom: 33%;" />

2. **MEMM**：MEMM模型是最大熵马尔可夫模型（Maximum Entropy Markov Model）的简称，是一种结合了最大熵模型和马尔可夫模型的概率图模型，用于序列标注等自然语言处理任务。MEMM模型使用有向图表示变量间的依赖关系，每个状态节点对应一个最大熵分类器，根据当前观察和前一个状态来预测下一个状态。
    - MEMM模型相比于HMM模型，有以下优点：
        - 可以利用更多的特征信息来建立条件概率分布，提高了模型的表达能力和泛化能力。
        - 可以使用前向后向算法进行高效的推理和学习，避免了HMM中的维特比算法可能出现的错误传播问题。
    - MEMM模型相比于CRF模型，有以下缺点：
        - 容易出现标注偏置问题（label bias problem），即某些状态节点由于连接较少或者转移概率较高而被过分偏好，导致预测结果不准确。
        - 训练过程计算量较大，因为需要定义大量的约束条件，并且约束条件与样本数目有关。



> （1）CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较 
>
> （2）同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较 
>
> （3）CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。 ————与ME比较 缺点：训练代价大、复杂度高

3. 【HMM】：如果已知观察序列和产生观察序列的状态序列,那么可用**极大似然估计**进行估计
    - 三个基本问题：
        - 评估—前向后向算法
        - 解码（预测）—维特比算法
        - 学习—Baum-Welch算法



3. 【朴素贝叶斯】：生成式模型、非线性模型

    - 他的一个假设是：特征变量X的各个维度是类别【**条件独立**】随机变量
    - 基于**先验概率**输出最大的后验概率：先验概率可以认为是事先已知的，后验概率为事先未知的条件分布。贝叶斯定理认为参数未知，需要求出参数，也就是定参

4. 【时间序列模型】

    - 广义回归模型（GARCH），对误差的方差建模，适用于波动性的分析和预测
    - AR模型：自回归模型，是一种线性模型
    - MA模型：移动平均法模型，其中使用趋势移动平均法建立直线趋势的预测模型
    - ARMA模型：自回归滑动平均模型，拟合较高阶模型

5. 【KNN】在样本较少但典型性好的条件下，效果较好（理解：给定的K样本均正确分类，此时性能较好）

6. 【SVM】SVM构造分类平面时，求解能够正确划分数据集，并且**几何间距最大**的超平面。

    - **分类间隔**为$2/\|w\|$，$\|w\|$ 代表向量的模
    - **核函数**包括：线性核、多项式核、高斯核、拉普拉斯核、Sigmoid核
    - 对噪声不鲁棒

7. 【GBDT】：GBDT是梯度提升决策树（Gradient Boosting Decision Tree）的简称，是一**种基于boosting集成学习思想的加法模型**，训练时采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。GBDT可以用于回归和分类问题，具有很好的泛化能力和解释性。简单来说，GBDT就是利**用多棵决策树来组合成一个强学习器，每一棵新树都试图修正上一棵树的错误，而修正的方向就是损失函数的负梯度方向**。

8. 【二次准则函数的H-K算法】：二次准则函数的H-K算法是一种基于最小均方误差（LSME）的**感知机学习**算法。

    该算法的基本思想是，对于**线性可分的训练样本**，求解一个最优权向量，使得感知机输出与期望输出之间的**均方误差最小**；对于<u>非线性可分的训练样本，判断出问题不可分，并退出迭代过程</u>。

    该算法的优点是，相对于感知机算法，其**适应性更好**，可以处理线性可分和非线性可分的情况；其缺点是，**计算量较大**，需要求解矩阵的逆。

9. 【集成学习算法】

    - Bagging：Bagging 是一种并行的集成方法，它通过**有放回地随机采样**得到多个训练集，然后用每个训练集独立地训练出一个基础模型，最后对所有基础模型进行**投票或平均**来得到最终的预测结果。Bagging 的目的是<u>降低方差，提高泛化能力</u>。
    - Boosting：Boosting 是一种**串行的集成方法**，它通过**逐步增加样本权重或学习率**来训练出一系列基础模型，然后用**加权投票或加权平均**来得到最终的预测结果。Boosting 的目的是<u>降低偏差，提高拟合能力</u>
        - 【GBDT】多棵决策树来组合成一个强学习器，每一棵新树都试图**修正上一棵树**的错误，而修正的方向就是损失函数的负梯度方向。
        - 【XGBoost对异常值敏感】
        - 【adaboost所有被分错的样本的权重更新比例相同】
    - Stacking：Stacking 是一种**层次化的集成方法**，它通过将多个不同类型的基础模型的**预测结果作为新的特征**输入到一个元模型中，从而得到最终的预测结果。Stacking 的目的是<u>提升预测准确性</u>
    - Blending：Blending 是一种简化版的 Stacking 方法，它通过将原始数据集分为两部分：训练集和测试集。然后用训练集训练出多个基础模型，并用这些模型对测试集进行预测。接着将测试集上的预测结果作为新特征与原始特征拼接起来，再用拼接后的数据训练一个元模型。Blending 的目的也是提升预测准确性。

10. 决策树：

    - ID3决策树是根据信息增益来划分属性

    - C4.5决策树是根据增益率来划分属性

    - CART决策树是根据基尼指数来划分属性

    - 基尼指数反映了从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此越小越好

        

11. 【N-gram算法】

    - N-gram的基本思想是将文本内容按字节流进行大小为N的滑动窗口操作，形成长度为N的字节片段序列，每个字节片段即为gram，对全部gram的出现频度进行统计，并按照设定的阈值进行过滤，形成key-gram列表，即为该文本的特征向量空间，每一种gram则为特征向量维度。其数学模型为
        $$
        P\left(w_1 w_2 w_3 \ldots w_n\right)=P\left(w_1\right) P\left(w_2 \mid w_1\right) P\left(w_3 \mid w_2, w_1\right) \ldots P\left(w_n \mid w_{n-1}, w_{n-2}, \ldots, w_1\right)
        $$
        <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230316141923572.png" alt="image-20230316141923572" style="zoom:33%;" />

        <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230316141901172.png" alt="image-20230316141901172" style="zoom:33%;" />

11. 【EM算法】：无监督算法
    - EM 算法通过逐步提高极大似然的下限，以此求出极大似然函数对参数的估计，为无监督算法
    - 已知样本数据服从K个概率分布，样本具体属于哪个概率分布未知，概率分布参数也未知。先初始化概率分布的参数，判断各个样本的归属。现在已知样本归属，根据样本数据重新计算参数
12. 【线性分类器】
    - 线性分类器有三大类：感知器准则函数、SVM、Fisher准则
    - **感知器准则函数**：准则函数以使错分类样本到分界面距离之和最小为原则。 其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 
    - **SVM**：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用**核函数**可解决非线性问题：高斯核、RBF核、） 
    - **Fisher准则**：根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵  Sw  和类间离散矩阵  Sb  实现。
13. 【脊回归模型 (Ridge Regression)】调整正则化参数λ，来调整模型复杂度。当λ增大时，模型复杂度降低，偏差增大，方差减小
14. 【线性回归】：基本假设包括随机干扰项是**均值为0的同方差**正态分布、在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量、可以用**DW检验残差是否存在序列相关性**
15. 【bootstrap数据】有放回地从总共N个样本中抽样n个样本
16. 【各个算法的损失函数】
    - 最小二乘-Square loss
    - SVM-Hinge Loss
    - Logistic Regression-（log-Loss）
    - AdaBoost-指数损失函数
17. 

### 1.3 机器学习常用操作

1. 【归一化处理】

    随机森林和GBDT不用做归一化处理

2. 【降维方法】

    - LASSO

    - 主成分分析法

    - 聚类分析

    - 线性判别法

    - 小波分析法

    - 拉普拉斯特征映射

3. 【正则化】

    - 正则化可以防止过拟合

    - L1正则化能得到稀疏解

    - L2正则化约束了解空间：最大化分类间隔，使得分类器拥有更强的泛化能力；得到平滑的权值

    - Dropout也是一种正则化方法

4. 【特征选择】可以用到的方法

    - 卡方

    - 信息增益

    - 平均互信息

    - 期望交叉熵

5. 【上采样、下采样】

    - 上采样：是**增加少数类别**的样本数量，使得正负样本比例接近平衡。常用的上采样方法有随机复制、SMOTE算法、ADASYN算法等。

    - 下采样：是**减少多数类别**的样本数量，使得正负样本比例接近平衡。常用的下采样方法有随机删除、EasyEnsemble算法、BalanceCascade算法等。

6. 【熵的计算】
    $$
    H(X) = -\sum_{i=1}^n p_i \log p_i
    $$
    其中，$X$是一个随机变量，$p_i$是$X$取第$i$个值的概率，$\log$是以2为底的对数函数。

    熵公式的含义是，随机变量$X$的平均信息量。熵越大，表示信息越不确定；熵越小，表示信息越确定。

7. 【损失函数】

    在统计模式识分类问题中，当先验概率未知时，可以使用（）

    - **N-P判决**：在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。
        - p(y)已知，直接使用贝叶斯公式求后验概率即可； 
        - p(y)未知，可以使用**聂曼-皮尔逊决策(N-P决策)**来计算决策面。 
    - **最小最大损失准则**：
        - **最大最小损失规则**主要就是使用解决**最小损失规则**时先验概率未知或难以计算的问题的。

8. 【频繁项挖掘】
    $$
    \begin{array}{rl}
    support(A) & = P(A) \\ 
    confidence(B|A) & = \frac{P(AB)}{P(A)} \\ 
    lift(A|B) & = \frac{confidence(B|A)}{support(B)} = \frac{P(AB)}{P(A)P(B)}
    \end{array}
    $$

9. 【卡方检验值】：卡方检验值是一种特征选择的算法，用于评估特征和类别之间的相关性。**卡方检验值越大，表示特征和类别之间的相关性越强，越有利于分类。**

### 1.3 数理知识

1. 线性规划：

    - 满足线性规划问题，所有约束条件的解称为**可行解**；

    - 在标准型中**只满足等式约束条件**并且令所有的**非基变量为0**，此时等式约束方程组有唯一解，再加上**非基变量取零的解**，为线性规划问题的**基本解**，也可称为基础解。
    - 满足非负约束条件的基本解称为**基本可行解**。

2. 线性规划问题的对偶：
    - 任何一个线性规划都存在对偶问题，对偶问题的对偶问题就是原问题。
    - 互为对偶的线性规划，**一个无最优解，另一个也无最优解**，但是一个**无可行解**，另一个可能**有可行解**
3. PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)
    - CDF是PDF在特定区间上的积分
    - PMF描述的是离散型随机变量在特定取值点的概率
    - 有一个分布的CDF函数H(x),则H(a)等于P(X<=a)

![image-20230316154813165](https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230316154813165.png)
